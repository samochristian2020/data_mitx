{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "mount_file_id": "1HszEMVVkMcw_auiGuW4Ijap4hXhiyZEZ",
      "authorship_tag": "ABX9TyO8vE2RIu4EqT4Y6dS3x80U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samochristian2020/data_mitx/blob/main/data354_model_ASR_wolof.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construction d'un model Automatic Speech recognition (__ASR__) de transcription en __wolof__ pour l'entreprise __data354.com__.\n",
        "\n",
        "### Plan de Travail\n",
        "> **1**.  Choisir un model ASR pre-entraine dans plusieurs langues a paufiner\n",
        " (fine-tuning) pour ASR en wolof grace aux methodes: \"transfer learning\" et \"\n",
        "  cross lingual training\".\n",
        ">  \n",
        "> **2**.   Preparer (preprocessing dataset for the chosen model) les clips audio pour qu'ils soit compatibles avec le model choisi.\n",
        "> \n",
        "> **3**.  Telecharger le dataset dans notre environement, puis en ecouter/visualiser quelques elements.\n",
        ">\n",
        "> **4**.  Effectuer encore quelques etapes de data cleaning\n",
        ">\n",
        "> **5**.  Construction d'un vocabulaire a inclure dans la CTC \"head\" qui sera utilisee par notre model pour \"decoder\" l'output du model en text-sequence.\n",
        ">\n",
        "> **6**.  Creation d' un objet \"DataCollator\" qui servira a uniformiser la longueur des inputs dans chaques \"batch\" envoye au model.\n",
        ">\n",
        "> **7**.  Construction d'une methode pour l'evaluation basee sur \"word error rate\" metric (elle sera utilisee uniquement sur le train split de notre dataset car le test split ne contient pas de transcription) \n",
        ">\n",
        "> **8**. Construction du pipeline d'entrainement\n",
        ">\n",
        "> **9**. Entrainement(fine-tuning) du model\n",
        ">\n",
        "> **10**. Evaluation du model "
      ],
      "metadata": {
        "id": "xxwjNZ8DO3OL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etape 1: \n",
        "Choisir un model ASR pre-entraine dans plusieurs langues a paufiner (fine-tuning) pour ASR en wolof grace aux methodes: \"transfer learning\" et \" cross lingual training\".\n",
        "\n",
        "\n",
        "Compte tenu de tache a accomplir a savoir la creation d'un model ASR pour la transcription en WOLOF, Nous avions le choix entre : \n",
        " - Entrainer un model from Scratch a l'aide de l 'une des diverses methodes de Natural Langages Processing NLP disponible \"__inclure quelques methodes NLP pour ASR__\" ce qui est une tache assez complexe car il faudrait alors choisir le model NLP le plus a meme de \"representer\" les specificites du language WOLOF, il faudrait egalement avoir assez donnees en WOLOF, tant en termes de quantite, qu en termes de diversite, et il faudrait enfin avoir assez de temps, (a compter en semaines) et de processing power (last generation GPU pour entrainer le model) ce qui implique un investissement financier consequent.\n",
        "\n",
        " - L'autre alternative, plus accessible consiste a tirer profit de l'un des models de dernieres generations adapte pour la tache a accomplir, que nous allons paufiner (fine-tune) a l'aide de notre petit dataset pour son utilisation en ASR WOLOF. Ceci grace aux techiques de Transfer-learning et Cross-lingual learning possible pour le model choisi.\n",
        "\n",
        "- Nous avons choisi le model, Wav2Vec2.0-XLSR concu et realise par le laboratoire de recherche en intelligence artificielle de Facebook en 2020. Pour plus de details: Voir le Blog Officiel du Laboratoire AI de  Facebook [ici.](https://colab.research.google.com/drive/1HszEMVVkMcw_auiGuW4Ijap4hXhiyZEZ#scrollTo=1cDQPxo0rG26&line=10&uniqifier=1)"
      ],
      "metadata": {
        "id": "1cDQPxo0rG26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Voici un petit apercu du model choisi tel decrit par ses concepteurs (en anglais):\n",
        " \n",
        "_Wav2Vec2 is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by Alexei Baevski, Michael Auli, and Alex Conneau.  Soon after the superior performance of Wav2Vec2 was demonstrated on the English ASR dataset LibriSpeech, *Facebook AI* presented XLSR-Wav2Vec2 (click [here](https://arxiv.org/abs/2006.13979)). XLSR stands for *cross-lingual  speech representations* and refers to XLSR-Wav2Vec2's ability to learn speech representations that are useful across multiple languages._\n",
        "\n",
        "*Similar to Wav2Vec2, XLSR-Wav2Vec2 learns powerful speech representations from hundreds of thousands of hours of speech in more than 50 languages of unlabeled speech.* \n",
        "\n",
        "![wav2vec2_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xlsr_wav2vec2.png)\n",
        "\n",
        "*The authors show for the first time that massively pretraining an ASR model on cross-lingual unlabeled speech data, followed by language-specific fine-tuning on very little labeled data achieves state-of-the-art results. See Table 1-5 of the official [paper](https://arxiv.org/pdf/2006.13979.pdf).*"
      ],
      "metadata": {
        "id": "uoeaPYylEcWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[voici la description du model choisi](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)"
      ],
      "metadata": {
        "id": "TGAknp3DKYfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etape 2: \n",
        "Preparer (preprocessing dataset for the chosen model) les clips audio pour qu'ils soit compatibles avec le model choisi."
      ],
      "metadata": {
        "id": "Tg8GPizWrmJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le model n'accepte en inputs que des fichiers audio de type `.wav` (raw waveform) avec un sample rate de 16kHz comme on peut le voir dans la fiche descriptive du modele.\n",
        "\n",
        "Mais notre dataset ne contient que des fichiers de type: `.mp3` (ainsi qu'on peut le voir dans fiche descriptive du challenge)  \n",
        "- Affichage d'un fichier audio arbitraire de notre dataset(voir l'extenxion) et ils sont formattes avec un sample rate de 48000Hz"
      ],
      "metadata": {
        "id": "nxwMHtxSAIUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import soundfile as sf\n",
        "\n",
        "audio_files = os.listdir(\"/content/drive/MyDrive/data354/clips/clips\")\n",
        "\n",
        "print(\"sample rate of a random audio signal before conversion is : \",sf.read(\"/content/drive/MyDrive/data354/clips/clips/\"+random.choice(audio_files))[1])\n",
        "print(\"file type of a random clip is : \", \"\\n\", random.choice(audio_files))\n"
      ],
      "metadata": {
        "id": "o1Tv5kCiNYdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"total number of audio clips is : \",len(audio_files))"
      ],
      "metadata": {
        "id": "743j5fY_xMpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans les cellules qui suivent, nous allons:\n",
        " - Convertir des fichiers `.mp3`en fichiers `.wav` et formater avec un sample de rate de 16kHz \n"
      ],
      "metadata": {
        "id": "CoejhuUFOyhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#library to process audio files \n",
        "%%capture\n",
        "!pip install pydub"
      ],
      "metadata": {
        "id": "M-DMAnBMHp7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhzpImXcMY-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# other utility libraries \n",
        "from tqdm.notebook import tqdm\n",
        "from pydub import AudioSegment \n",
        "from joblib import Parallel, delayed\n"
      ],
      "metadata": {
        "id": "yhe5D0ruFhBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgRnlHN3UREg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ROOT_PATH is the path to my (google drive) folder containing original data\n",
        "ROOT_PATH = \"/content/drive/MyDrive/data354/clips/clips\"\n",
        "\n",
        "#creating a temporary folder to store our wav_data\n",
        "!mkdir -p \"/tmp/data354_WOLOF_ASR_dataset/audio_wav_16000\"\n",
        "\n",
        "OUTPUT_DIR = \"/tmp/data354_WOLOF_ASR_dataset/audio_wav_16000\"\n",
        "            "
      ],
      "metadata": {
        "id": "3GZX6yPLIh56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rge0W51ftrKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conversion function\n",
        "# def save_fn(filename):\n",
        "    \n",
        "#     path = f\"{ROOT_PATH}/{filename}\"\n",
        "#     save_path = f\"{OUTPUT_DIR}\"\n",
        "#     if not os.path.exists(save_path):\n",
        "#         os.makedirs(save_path, exist_ok=True)\n",
        "    \n",
        "#     if os.path.exists(path):\n",
        "#         try:\n",
        "#             sound = AudioSegment.from_mp3(path)\n",
        "#             sound = sound.set_frame_rate(16000)\n",
        "#             sound.export(f\"{save_path}/{filename[:-4]}.wav\", format=\"wav\")\n",
        "#         except:\n",
        "#             pass\n"
      ],
      "metadata": {
        "id": "3j09YEH9s64T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parallelizing the task\n",
        "#this takes approximately 20-35min\n",
        "# %%capture\n",
        "# Parallel(n_jobs=8, backend=\"multiprocessing\")(delayed(save_fn)(filename) for filename in tqdm(audio_files))"
      ],
      "metadata": {
        "id": "o25H-8l1LA7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#verifying that conversion succeeded  (format and sample rate)\n",
        "audio_files_wav = os.listdir(OUTPUT_DIR)\n",
        "\n",
        "print(\"the sample rate of a random audio signal after the conversion to wave file format is : \",sf.read(OUTPUT_DIR+\"/\"+random.choice(audio_files_wav))[1])\n",
        "print(\"file type of a random clip is :\", \"\\n\",random.choice(audio_files_wav))"
      ],
      "metadata": {
        "id": "z1mnz1AJK6KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NXtmamTjk--M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R1REG8zuT7fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etape 3\n",
        "Telecharger le dataset dans notre environement, puis en ecouter/visualiser quelques elements."
      ],
      "metadata": {
        "id": "EYSJZbDZr5dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#train dataset\n",
        "df_train = pd.read_csv(\"/content/drive/MyDrive/data354/train.csv\")\n",
        "\n",
        "#test dataset\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/data354/test.csv\")\n",
        "\n",
        "df_train, df_eval = train_test_split(df_train, test_size=0.01, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "HFldJh2ep4H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_train)\n"
      ],
      "metadata": {
        "id": "9lCJdaRCudlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "display(df_eval)\n",
        "print(\"eval dataframe shape is :\", df_eval.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "xzntHjtkMmOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fisrt let's listen to some random files \n",
        "# and also pull out their written transciption when they are in either of the train or test splits\n",
        "\n",
        "## !! some files are in neither splits, so it will be no written transciptions for those !!\n",
        "\n",
        "import IPython.display as ipd\n",
        "\n",
        "\n",
        "rand_file = random.choice(audio_files_wav)\n",
        "\n",
        "speech_array, sampling_rate = sf.read(OUTPUT_DIR+\"/\"+rand_file)\n",
        "\n",
        "rand_file_stripped = rand_file.strip(\".wav\")\n",
        "\n",
        "\n",
        "if not rand_file_stripped in df_test[\"ID\"]:\n",
        "  print(df_train[df_train[\"ID\"]==rand_file_stripped][\"transcription\"],\"\\n\")\n",
        "elif not rand_file_stripped in df_eval[\"ID\"]:\n",
        "  print(df_test[df_test[\"ID\"]==rand_file_stripped][\"transcription\"],\"\\n\")\n",
        "else:\n",
        "  print(df_eval[df_eval[\"ID\"]==rand_file_stripped][\"transcription\"],\"\\n\")\n",
        "\n",
        "ipd.Audio(data=speech_array, autoplay=True, rate=16000)"
      ],
      "metadata": {
        "id": "bVLvJUOfsKxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NlFBQjfqplGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(df_train.shape, df_eval.shape, df_test.shape)"
      ],
      "metadata": {
        "id": "NKTbzQgdplA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "QuLtnSpnp7qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#creating a list with first five clips the dataset\n",
        "speech_deb = []\n",
        "for i in [3829,3688]:\n",
        "  speech_deb.append(sf.read(OUTPUT_DIR+\"/\"+df_train[\"ID\"][i]+\".wav\")[0])\n",
        "\n",
        "\n",
        "#creating a list with last five clips in the dataset\n",
        "speech_fin = []\n",
        "for i in [5226,860]:\n",
        "  speech_fin.append(sf.read(OUTPUT_DIR+\"/\"+df_train[\"ID\"][i]+\".wav\")[0])"
      ],
      "metadata": {
        "id": "zl3uKSgG3XGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HL87OEt2AuUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first two element in the train set\n",
        "display(df_train.head(2))"
      ],
      "metadata": {
        "id": "SGFTKqx8As77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eals1CgmDh5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audio clip corresponding to the 1st element in the train set\n",
        "\n",
        "print(df_train.loc[3829][\"transcription\"],\"\\n\")\n",
        "display(ipd.Audio(data=speech_deb[0], autoplay=True, rate=16000))"
      ],
      "metadata": {
        "id": "1mF7kKsc4lA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audio clip corresponding to the 2nd element in the train set\n",
        "print(df_train.loc[3688][\"transcription\"],\"\\n\")\n",
        "display(ipd.Audio(data=speech_deb[1], autoplay=True, rate=16000))"
      ],
      "metadata": {
        "id": "WhLwRtgd_feS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fXzkLQe_eXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# last two element in the train set\n",
        "display(df_train.tail(2))"
      ],
      "metadata": {
        "id": "phPavPdt_4Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JClR2bMy_3ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audio clip corresponding to the 2nd last element in the train set\n",
        "print(df_train.loc[5226][\"transcription\"],\"\\n\")\n",
        "display(ipd.Audio(data=speech_fin[0], autoplay=True, rate=16000))"
      ],
      "metadata": {
        "id": "4InIahAd_3eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audio clip corresponding to the last element in the train set\n",
        "print(df_train.loc[860][\"transcription\"],\"\\n\")\n",
        "display(ipd.Audio(data=speech_fin[1], autoplay=True, rate=16000))"
      ],
      "metadata": {
        "id": "IJ9y--AI_3VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pblCJCvx2Y6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding the path to each clip in the train set and the test set \n",
        "df_train[\"clip_path\"] = OUTPUT_DIR+\"/\"+df_train[\"ID\"]+\".wav\"\n",
        "df_test[\"clip_path\"] = OUTPUT_DIR+\"/\"+df_test[\"ID\"]+\".wav\"\n"
      ],
      "metadata": {
        "id": "L_kzUQ4cpksl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_eval[\"clip_path\"] = OUTPUT_DIR+\"/\"+df_eval[\"ID\"]+\".wav\"\n"
      ],
      "metadata": {
        "id": "IIrM7eI3NUJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_train)\n"
      ],
      "metadata": {
        "id": "3BduxrXdp3ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_test)"
      ],
      "metadata": {
        "id": "ULbOnn_6rN_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y0ciTEKOEVLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etape 4\n",
        " Effectuer encore quelques etapes pour la preparation des donnees"
      ],
      "metadata": {
        "id": "53swuxFmr6LN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets==1.13.3\n",
        "!pip install transformers==4.11.3\n",
        "!pip install torchaudio\n",
        "!pip install librosa\n",
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "vI-PEbIbsXnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ubq7lsyK5cdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset "
      ],
      "metadata": {
        "id": "RRRyUg2USvQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = Dataset.from_pandas(df_train)\n",
        "data_eval = Dataset.from_pandas(df_eval)\n",
        "\n",
        "data_test = Dataset.from_pandas(df_test)\n"
      ],
      "metadata": {
        "id": "pQszEdIrSu3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zz-SnfPVWmGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fonction to add numpy array of the audio clips to the train and test datasets \n",
        "import numpy as np\n",
        "import torchaudio\n",
        "\n",
        "resampler  = torchaudio.transforms.Resample(48000, 16000)\n",
        "def fct_speech_file_to_array_train(batch):\n",
        "  speech_array, sampling_rate = torchaudio.load(batch[\"clip_path\"])\n",
        "  batch[\"audio_array\"] = resampler(speech_array).squeeze().numpy()\n",
        "  batch[\"sampling_rate\"] = sampling_rate\n",
        "  batch[\"target_text\"] = batch[\"transcription\"]\n",
        "\n",
        "  return batch\n",
        "    \n",
        "\n",
        "def fct_speech_file_to_array_test(batch):\n",
        "  speech_array, sampling_rate = torchaudio.load(batch[\"clip_path\"])\n",
        "  batch[\"audio_array\"] = speech_array.squeeze().numpy()\n",
        "  batch[\"sampling_rate\"] = sampling_rate\n",
        "  \n",
        "  return batch"
      ],
      "metadata": {
        "id": "522WGdQ4R4UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "OTkFFPu6tzbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_train = data_train.remove_columns([\"ID\" ])\n",
        "\n",
        "data_eval = data_eval.remove_columns([\"ID\" ])\n",
        "\n",
        "\n",
        "data_test = data_test.remove_columns([\"ID\" ])\n"
      ],
      "metadata": {
        "id": "yYZhxXP3ZDj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.column_names)\n",
        "print(data_test.column_names)"
      ],
      "metadata": {
        "id": "_nkbDMh0rWEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_train = data_train.map(fct_speech_file_to_array_train, remove_columns = data_train.column_names, num_proc=4)\n",
        "data_eval = data_eval.map(fct_speech_file_to_array_train, remove_columns = data_eval.column_names, num_proc=4)\n",
        "\n",
        "\n",
        "\n",
        "data_test = data_test.map(fct_speech_file_to_array_test, remove_columns = data_test.column_names, num_proc=4)\n"
      ],
      "metadata": {
        "id": "a-WMPxKpak1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "g174Dh-1akb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qIwsLg35orV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "id": "jkThPwsDnfmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "id": "JCuwVpcInrix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D3cynSXabTce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "rand_int = random.randint(0, len(data_train)-1)\n",
        "\n",
        "print(\"Transcription:\", data_train[rand_int][\"target_text\"])\n",
        "print(\"audio array :\", data_train[rand_int][\"audio_array\"])\n",
        "#print(\"audio array type is :\", type(data_train[rand_int][\"audio_array\"]))\n",
        "print(\"Sampling rate:\", data_train[rand_int][\"sampling_rate\"])"
      ],
      "metadata": {
        "id": "slkA84LeR3kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jc73OtbZR3cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "j8ijT281Y6P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9JPV-h6dZQQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Mlf-16WR3HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGXoC5Se5cWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u45KaUgU5cOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etape 5\n",
        "Construction d'un vocabulaire a inclure dans la CTC \"head\" qui sera utilisee par notre model pour \"decoder\" l'output du model en text-sequence."
      ],
      "metadata": {
        "id": "lMnoD0tCr6yM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jGy_zbH6FB80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"target_text\"] = re.sub(chars_to_ignore_regex, '', batch[\"target_text\"]).lower() + \" \"\n",
        "    return batch"
      ],
      "metadata": {
        "id": "FZKa4frvFBqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KxTiMC5-FMBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_train = data_train.map(remove_special_characters)\n",
        "\n",
        "data_eval = data_eval.map(remove_special_characters)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EXzFF2C2FOFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "efgVYeelFNx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_chars(batch):\n",
        "  all_text = \" \".join(batch[\"target_text\"])\n",
        "  vocab = list(set(all_text))\n",
        "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
      ],
      "metadata": {
        "id": "kHOEuBU_Cwg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zr89KkRzFxDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_sharded_1 = data_train.shard(num_shards=5, index=0) \n",
        "\n",
        "data_train_sharded_2 = data_train.shard(num_shards=5, index=1) \n",
        "\n",
        "data_train_sharded_3 = data_train.shard(num_shards=5, index=2) \n",
        "\n",
        "data_train_sharded_4 = data_train.shard(num_shards=5, index=3) \n",
        "\n",
        "data_train_sharded_5 = data_train.shard(num_shards=5, index=4) \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bup2C0B3f-kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GJBVlLVMgD29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_sharded_1 = data_test.shard(num_shards=2, index=0) \n",
        "\n",
        "data_test_sharded_2 = data_test.shard(num_shards=2, index=1) "
      ],
      "metadata": {
        "id": "ifLQclwdf_2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RyCd5RKiK_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_train_1 = data_train_sharded_1.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=data_train_sharded_1.column_names)\n",
        "\n",
        "vocab_train_2 = data_train_sharded_2.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=data_train_sharded_2.column_names)\n",
        "\n",
        "vocab_train_3 = data_train_sharded_3.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=data_train_sharded_3.column_names)\n",
        "\n",
        "vocab_train_4 = data_train_sharded_4.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=data_train_sharded_4.column_names)\n",
        "\n",
        "vocab_train_5 = data_train_sharded_5.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=data_train_sharded_5.column_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mpuk3BGjFBWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_eval = data_eval.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=data_eval.column_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "kQMVppEUCrbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29_CHZNQkTb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list = list(set(vocab_eval[\"vocab\"][0]) | set(vocab_train_1[\"vocab\"][0]) |set(vocab_train_2[\"vocab\"][0]) | set(vocab_train_3[\"vocab\"][0]) |set(vocab_train_4[\"vocab\"][0]) | set(vocab_train_5[\"vocab\"][0]) )"
      ],
      "metadata": {
        "id": "WRDv4h91kUWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VbgO_78CkTGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
        "vocab_dict"
      ],
      "metadata": {
        "id": "lAVJmkyYDunJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yOEnrhxqFLiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
        "del vocab_dict[\" \"]\n",
        "\n",
        "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
        "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
        "len(vocab_dict)"
      ],
      "metadata": {
        "id": "1a1VfXMeFLK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)"
      ],
      "metadata": {
        "id": "bTLvDgZ-FK0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
        "\n",
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
        "\n",
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "DKph6fA5FKbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_nkjSwbF5qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NkI9sMXEF5av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # check that all files have the correct sampling rate\n",
        "    assert (len(set(batch[\"sampling_rate\"])) == 1), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
        "\n",
        "    batch[\"input_values\"] = processor(batch[\"audio_array\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
        "    \n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "58vrA1HxF5Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iC5H73t_63-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset_test(batch):\n",
        "    # check that all files have the correct sampling rate\n",
        "    assert (\n",
        "        len(set(batch[\"sampling_rate\"])) == 1\n",
        "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
        "\n",
        "    batch[\"input_values\"] = processor(batch[\"audio_array\"], padding=True,sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
        "    \n",
        "    return batch"
      ],
      "metadata": {
        "id": "E9VUleq663bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "data_train_sharded_1 = data_train_sharded_1.map(prepare_dataset, remove_columns=data_train_sharded_1.column_names, num_proc=4, batched = True, batch_size = -1)\n",
        "\n",
        "data_train_sharded_2 = data_train_sharded_2.map(prepare_dataset, remove_columns=data_train_sharded_2.column_names, num_proc=4, batched = True, batch_size = -1)\n",
        "\n",
        "data_train_sharded_3 = data_train_sharded_3.map(prepare_dataset, remove_columns=data_train_sharded_3.column_names, num_proc=4, batched = True, batch_size = -1)\n",
        "\n",
        "data_train_sharded_4 = data_train_sharded_4.map(prepare_dataset, remove_columns=data_train_sharded_4.column_names, num_proc=4, batched = True, batch_size = -1)\n",
        "\n",
        "data_train_sharded_5 = data_train_sharded_5.map(prepare_dataset, remove_columns=data_train_sharded_5.column_names, num_proc=4, batched = True, batch_size = -1)\n",
        "\n"
      ],
      "metadata": {
        "id": "m620mFT5F7vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "znh-06GoOj-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_eval = data_eval.map(prepare_dataset, remove_columns=data_eval.column_names, batch_size=8, num_proc=4, batched=True)\n"
      ],
      "metadata": {
        "id": "mgRvM_ZZOjqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZvHq7-eGLJFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_test_sharded_1 = data_test_sharded_1.map(prepare_dataset_test, remove_columns=data_test_sharded_1.column_names, batch_size=-1, num_proc=4, batched=True)\n",
        "\n",
        "data_test_sharded_2 = data_test_sharded_2.map(prepare_dataset_test, remove_columns=data_test_sharded_2.column_names, batch_size=-1, num_proc=4, batched=True)\n"
      ],
      "metadata": {
        "id": "gTK4lqO9fh8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yy8C_DxCF7nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "data_train_ready = concatenate_datasets([data_train_sharded_1, data_train_sharded_2, data_train_sharded_3, data_train_sharded_4, data_train_sharded_5])"
      ],
      "metadata": {
        "id": "D4PZHgyqeytx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_ready"
      ],
      "metadata": {
        "id": "ZbV_BHZu8jsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_eval_ready = data_eval"
      ],
      "metadata": {
        "id": "vQ8LeF_dOMmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_ready = concatenate_datasets([data_train_sharded_1, data_train_sharded_2])"
      ],
      "metadata": {
        "id": "T_g8dy1gfWg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_ready"
      ],
      "metadata": {
        "id": "WSkUl6eyF5Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_ready[0][\"labels\"][:10]"
      ],
      "metadata": {
        "id": "x5ynWF5lbU3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etape 6\n",
        " Creation d' un objet \"DataCollator\" qui servira a uniformiser la longueur des inputs dans chaques \"batch\" envoye au model."
      ],
      "metadata": {
        "id": "-PFBt3bar7Re"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xg9WA4Tps62s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n"
      ],
      "metadata": {
        "id": "fL6mjqs80TRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ],
      "metadata": {
        "id": "cG9pSteH0TMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Qgii4nT0TGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Aaaa83A0TBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJdxna6-0SlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etape 7\n",
        " Construction d'une methode pour l'evaluation basee sur \"word error rate\" metric (elle sera utilisee uniquement sur le train split de notre dataset car le test split ne contient pas de transcription) "
      ],
      "metadata": {
        "id": "kDRnRflBr7sP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n9ANWPUP0oN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "\n",
        "wer_metric = load_metric(\"wer\")\n"
      ],
      "metadata": {
        "id": "NDfTVo7A0n_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "usOPtOJos6El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etape 8: Construction du pipeline d'entrainement"
      ],
      "metadata": {
        "id": "AhMPmVI4r8GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vRkLuRbS0zy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC\n",
        "Wav2Vec2ForCTC\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-large-xlsr-53\", \n",
        "    attention_dropout=0.1,\n",
        "    hidden_dropout=0.1,\n",
        "    feat_proj_dropout=0.0,\n",
        "    mask_time_prob=0.05,\n",
        "    layerdrop=0.1,\n",
        "    gradient_checkpointing=True,\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer)\n",
        ")"
      ],
      "metadata": {
        "id": "TeeG1Fqa0zu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.freeze_feature_extractor()"
      ],
      "metadata": {
        "id": "vO6gp0Ym0zq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "BdXpw89mdAMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-WOLOF\",\n",
        "  #output_dir=\"./wav2vec2-large-xlsr-WOLOF\",\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=16,\n",
        "  gradient_accumulation_steps=2,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=30,\n",
        "  fp16=True,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=3e-4,\n",
        "  warmup_steps=500,\n",
        "  save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "id": "Yzjd-NN-0zmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=data_train_ready,\n",
        "    eval_dataset=data_eval_ready,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "VrkgEDsB0zhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N91cTLris8c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etape 9: Entrainement (fine-tuning) du model\n"
      ],
      "metadata": {
        "id": "-_VmhoUgr8j-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kq6EzuIk1MIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "\n"
      ],
      "metadata": {
        "id": "m8Pne6671MD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "91jZfjlS1MAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"wav2vec2-large-xlsr-WOLOF\")\n",
        "processor.save_pretrained(\"wav2vec2-large-xlsr-WOLOF\")"
      ],
      "metadata": {
        "id": "PwV5R5rLs9C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etape 10: Evaluation du model "
      ],
      "metadata": {
        "id": "HEDbX_pKr-vR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8uwWgtxs90T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val =pd.read_csv(\"../input/wolof-asr/Test.csv\")\n",
        "val[\"path\"] = \"../input/wolof-asr/Noise Removed/tmp/WOLOF_ASR_dataset/noise_remove/\"+val[\"ID\"]+\".wav\"\n",
        "val.rename(columns = {'transcription':'sentence'}, inplace = True)\n",
        "common_voice_val = Dataset.from_pandas(val)"
      ],
      "metadata": {
        "id": "Uy05mBh5120W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice_val = common_voice_val.remove_columns([ \"ID\",\"age\",  \"down_votes\", \"gender\",  \"up_votes\"])"
      ],
      "metadata": {
        "id": "LY1wnIv612wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice_val = common_voice_val.map(speech_file_to_array_fn_test, remove_columns=common_voice_val.column_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "wQf0D1ke12sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice_val = common_voice_val.map(prepare_dataset_test, remove_columns=common_voice_val.column_names, batch_size=8, num_proc=4, batched=True)"
      ],
      "metadata": {
        "id": "piBXhx-x12nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a6NVi62Q12iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_pred = []\n",
        "for i in tqdm(range(data_test_ready.shape[0])):    \n",
        "    input_dict = processor(data_test_ready[i][\"input_values\"], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1)[0]\n",
        "    prediction = processor.decode(pred_ids)\n",
        "    final_pred.append(prediction)"
      ],
      "metadata": {
        "id": "sSt9j1aL2GNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val[\"transcription\"] = final_pred\n",
        "val[\"transcription\"] = val[\"transcription\"].str.capitalize()\n",
        "val.iloc[1390,6] = \"ah\""
      ],
      "metadata": {
        "id": "6xZhWKYJ2GIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqTNFqcI2Vj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val[[\"ID\",\"transcription\"]].to_csv(\"submission_file.csv\", index=False)"
      ],
      "metadata": {
        "id": "HPMHhVoH2Vfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val[\"transcription\"] "
      ],
      "metadata": {
        "id": "byzi6Wzq2VbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gMl0d_bd2VXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AEYuHdKh2GET"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}